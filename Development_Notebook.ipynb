{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Either fill out the config files in this folder before the run, or you can manually update the varoious config dictionaries with hardcoded values in this jupyter notebook. \n",
    "\n",
    "**My recommendation:** Fill out the config files first and then you can make the iterative and interactive changes in this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pymysql\n",
    "import time\n",
    "import datetime as dt\n",
    "import gadgets as gd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import pymysql\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus  import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import GRU, RNN, Conv1D, GlobalAveragePooling1D\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Flatten, LSTM, Lambda, Embedding,concatenate \n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop,Adagrad,Adamax,Nadam\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau,LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "\n",
    "jobs = 20\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=jobs,\n",
    "                       inter_op_parallelism_threads=jobs,\n",
    "                       allow_soft_placement=True,\n",
    "                       device_count={'CPU':jobs})\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = None\n",
    "with open(\"read_data_config.yml\", \"r\") as config_file:\n",
    "    data_config = yaml.safe_load(config_file)\n",
    "\n",
    "model_info_config = None\n",
    "with open(\"model_info_table_config.yml\", \"r\") as config_file:\n",
    "    model_info_config = yaml.safe_load(config_file)\n",
    "\n",
    "model_config = None\n",
    "with open(\"train_model_config.yml\", \"r\") as config_file:\n",
    "    model_config = yaml.safe_load(config_file)\n",
    "\n",
    "pipeline_config = None\n",
    "with open(\"run_model_pipeline_config.yml\", \"r\") as config_file:\n",
    "    pipeline_config = yaml.safe_load(config_file)\n",
    "\n",
    "process_config = None\n",
    "with open(\"process_data_config.yml\", \"r\") as config_file:\n",
    "    process_config = yaml.safe_load(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates folder structure for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folder_structure(model_train_date):\n",
    "    model_data_base_path = (\n",
    "        os.getcwd() + '/models/' \n",
    "    )\n",
    "    model_folder = \"_\".join(str(dt.datetime.now()).split(\" \"))\n",
    "    model_full_path = model_data_base_path + model_folder\n",
    "    \n",
    "    if not os.path.isdir(model_data_base_path):\n",
    "        os.mkdir(model_data_base_path)\n",
    "    \n",
    "    os.mkdir(model_full_path)\n",
    "    os.mkdir(model_full_path + \"/encoders\")\n",
    "    os.mkdir(model_full_path + \"/scalers\")\n",
    "    os.mkdir(model_full_path + \"/imputers\")\n",
    "    os.mkdir(model_full_path + \"/tokenizers\")\n",
    "       \n",
    "    return model_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_date = str(dt.datetime.now())\n",
    "model_expires_date = str(\n",
    "    (dt.datetime.now() + dt.timedelta(days = pipeline_config['model_train_interval'])\n",
    ").date())\n",
    "# create path to save stuff in\n",
    "model_path = make_folder_structure(model_train_date)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to Hardcode training, test, validation dates, run this next cell. (if skipped, the config file options will be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = {}\n",
    "date_dict['prediction_date_end'] = dt.date(2021, 3, 5)\n",
    "date_dict['prediction_date_start'] = dt.date(2021, 3, 2)\n",
    "\n",
    "date_dict['validation_date_end'] = dt.date(2021, 3, 1)\n",
    "date_dict['validation_date_start'] = dt.date(2021, 2, 27)\n",
    "\n",
    "date_dict['train_date_end'] = dt.date(2021, 2, 28)\n",
    "date_dict['train_date_start'] = dt.date(2021, 2, 10)\n",
    "\n",
    "process_config = {**process_config, **date_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility functions for reading data below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_formatter(**kwargs):\n",
    "    return data_config[\"sme_query_template\"].format(**kwargs)\n",
    "\n",
    "def connect_db(db_dict):\n",
    "    return pymysql.connect(**db_dict)\n",
    "\n",
    "def make_date_list(start_date: str, end_date: str, format='%Y-%m-%d'):\n",
    "    start_date = dt.datetime.strptime(start_date, format).strftime(\n",
    "        '%Y-%m-%d'\n",
    "    )\n",
    "    end_date = dt.datetime.strptime(end_date, format).strftime(\n",
    "        '%Y-%m-%d'\n",
    "    )\n",
    "    start_end = (dt.datetime.strptime(end_date, format)\n",
    "                 - dt.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    end_start = (dt.datetime.strptime(start_date, format)\n",
    "                 + dt.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "    date_range = np.concatenate(\n",
    "        [np.arange(start_date,\n",
    "                   start_end,\n",
    "                   dtype='datetime64[D]').reshape(-1, 1),\n",
    "         np.arange(end_start,\n",
    "                   end_date,\n",
    "                   dtype='datetime64[D]').reshape(-1, 1)],\n",
    "        axis=1)\n",
    "\n",
    "    return date_range\n",
    "\n",
    "def make_date_list_from_datetime(start_date, end_date):\n",
    "    return make_date_list(str(start_date), str(end_date + dt.timedelta(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Makes the data dictionary that is used in formatting the multi-threaded sme queries\n",
    "'''\n",
    "def make_data_dict(start_date, end_date):\n",
    "    sql_list_str = lambda l: \",\\n\".join(l)\n",
    "    list_str = lambda l: '\"' + '\",\\n\"'.join(l) + '\"'\n",
    "    data_dict = {\n",
    "        **data_config,\n",
    "        \"date_start\": start_date,\n",
    "        \"date_end\": end_date,\n",
    "        \"programid_list_str\" : list_str(data_config['inscope_programs']),\n",
    "        \"feature_list_str\": sql_list_str(data_config['raw_feature_list']),\n",
    "        \"eng_feature_list_str\": sql_list_str(data_config['eng_feature_list'])\n",
    "    }\n",
    "    data_dict['sme_query'] = query_formatter(**data_dict)\n",
    "    data_dict['date_list'] = make_date_list_from_datetime(start_date, end_date)\n",
    "    data_dict['db_connector'] = lambda **kwargs: connect_db(data_dict['db_args'])\n",
    "    return data_dict\n",
    "'''\n",
    "Runs multi-threaded sme queries based on the given data dictionary\n",
    "'''\n",
    "def read_data(data_dict):\n",
    "    production_reader = gd.sql.MultiSQL(connector=data_dict['db_connector'])\n",
    "    print(\">>> start: {}\".format(time.asctime()))\n",
    "    data = gd.sql.trans.lower_columns(\n",
    "    production_reader.get_data(\n",
    "        query_gen=gd.sql.make_query_gen(\n",
    "            data_dict['sme_query'],\n",
    "            vals=data_dict['date_list']),\n",
    "        threads = data_dict['read_threads']\n",
    "    ))\n",
    "    print(\"<<< finished sme read: {}\".format(time.asctime()))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reads data and saves it into All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "all_data = read_data(make_data_dict(process_config['train_date_start'], process_config['prediction_date_end']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Indexes data based on index specified in config files. can be multi-index.\n",
    "1. Splits the data into train, validation, test\n",
    "2. Splits data by type (numerical, categorical, text, utility) as specified in the config file\n",
    "3. Imputes numerical data, scales numerical data using the Standard scaler\n",
    "4. Imputes missing categories with an 'NL' for no label. Creates smalls for categorical variables, then One Hot Encodes the non-smalls categories.\n",
    "5. Cleans text fields (keeps only alphanumeric characters), removes stop words, stems, tokenizes\n",
    "6. utility features are skipped over and attached after other data processed. (these are things like vdn, on_off, v_calltime_dt, programid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_rare_labels(df, variable, tolerance):\n",
    "    temp = df.groupby([variable])[variable].count() / len(df)\n",
    "    non_rare = [x for x in temp.loc[temp>tolerance].index.values]\n",
    "    return non_rare\n",
    "\n",
    "\n",
    "def remove_dates(df, params):\n",
    "    df = df.loc[-df[params['date_feature']].astype('str').isin(params['excluded_dates']), :]\n",
    "    return df\n",
    "\n",
    "def split_by_date(df, params, drop_date = True):  \n",
    "    \n",
    "    if 'test_date_start' not in params.keys() and 'prediction_date_start' in params.keys():\n",
    "        params['test_date_start'] = params['prediction_date_start']\n",
    "    if 'test_date_end' not in params.keys() and 'prediction_date_end' in params.keys():\n",
    "        params['test_date_end'] = params['prediction_date_end']\n",
    "    \n",
    "    \n",
    "    df_train = df[\n",
    "        (df[params['date_feature']] >= str(params['train_date_start']))\n",
    "        & (df[params['date_feature']] <= str(params['train_date_end']))\n",
    "    ].copy()\n",
    "    \n",
    "    df_val = df[\n",
    "        (df[params['date_feature']] >= str(params['validation_date_start']))\n",
    "        & (df[params['date_feature']] <= str(params['validation_date_end']))\n",
    "    ].copy()\n",
    "    \n",
    "    df_test = df[\n",
    "        (df[params['date_feature']] >= str(params['test_date_start']))\n",
    "        & (df[params['date_feature']] <= str(params['test_date_end']))\n",
    "    ].copy()\n",
    "    \n",
    "    if drop_date:\n",
    "        df_train.drop(params['date_feature'], axis = 1, inplace = True)\n",
    "        df_val.drop(params['date_feature'], axis = 1, inplace = True)\n",
    "        df_test.drop(params['date_feature'], axis = 1, inplace = True)\n",
    "        \n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "def append_smalls(categories):\n",
    "    if 'smalls' not in categories:\n",
    "        categories = np.array([*categories, 'smalls'], dtype = categories.dtype)\n",
    "    return categories\n",
    "    \n",
    "def process_num_preds(num, params, model_path):\n",
    "    sc = None\n",
    "    with open(model_path + '/scalers/standard_scaler.pickle', 'rb') as handle:\n",
    "        sc = pickle.load(handle)\n",
    "    num = num.replace('NA', np.nan)\n",
    "    num = num.replace('', np.nan)\n",
    "    num.drop(params['date_feature'], axis = 1, inplace = True)\n",
    "    for col in num.columns:\n",
    "        if col != params['date_feature']:\n",
    "            try:\n",
    "                mode = None\n",
    "                with open(model_path + '/imputers/mode_' + col +'.pickle', 'rb') as handle:\n",
    "                    mode = pickle.load(handle)\n",
    "\n",
    "                num[col] = num[col].fillna(mode) \n",
    "                num[col] = num[col].astype(np.float32)\n",
    "            except:\n",
    "                num.drop([col], axis = 1, inplace = True)\n",
    "                print('{} dropped!'.format(col))\n",
    "    \n",
    "    \n",
    "    num_cols = num.columns\n",
    "    df_num = sc.transform(num)\n",
    "    df_num = pd.DataFrame(df_num, columns = num_cols, index = num.index)\n",
    "    \n",
    "    return df_num\n",
    "\n",
    "def process_num(num, params, model_path):\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    num = num.replace('NA', np.nan)\n",
    "    num = num.replace('', np.nan)\n",
    "    \n",
    "    for col in num.columns:\n",
    "        if col != params['date_feature']:\n",
    "            try:\n",
    "                num[col] = num[col].astype(np.float32)\n",
    "                mode = num[col].mode()[0]\n",
    "                num[col] = num[col].fillna(mode)\n",
    "                with open(model_path + '/imputers/mode_' + col +'.pickle', 'wb') as handle:\n",
    "                    pickle.dump(mode, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "            except:\n",
    "                num.drop([col], axis = 1, inplace = True)\n",
    "                print('{} dropped!'.format(col))\n",
    "    \n",
    "    \n",
    "    num_train, num_val, num_test = split_by_date(num, params)\n",
    "    num_cols = num_train.columns\n",
    "    \n",
    "    sc.fit(num_train)\n",
    "\n",
    "    df_train_num = sc.transform(num_train)\n",
    "    df_val_num = sc.transform(num_val)\n",
    "    df_test_num = sc.transform(num_test)\n",
    "\n",
    "    df_train_num = pd.DataFrame(df_train_num, columns = num_cols, index = num_train.index)\n",
    "    df_val_num = pd.DataFrame(df_val_num, columns = num_cols, index = num_val.index)\n",
    "    df_test_num = pd.DataFrame(df_test_num, columns = num_cols, index = num_test.index)\n",
    "    \n",
    "    with open(model_path + '/scalers/standard_scaler.pickle', 'wb') as handle:\n",
    "        pickle.dump(sc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    return df_train_num, df_val_num, df_test_num\n",
    "\n",
    "def process_cat_preds(cat, params, model_path):\n",
    "    ohe = None\n",
    "    with open(model_path + '/encoders/one_hot_encoder.pickle', 'rb') as handle:\n",
    "        ohe = pickle.load(handle)\n",
    "    \n",
    "    cat = cat.replace('NA', np.nan)\n",
    "    cat = cat.replace('', np.nan)\n",
    "    cat = cat.replace(np.nan, 'NL')\n",
    "    cat.drop(params['date_feature'], axis = 1, inplace = True)\n",
    "    \n",
    "    for col in cat.columns:\n",
    "        if col != params['date_feature']:\n",
    "            frequent_cat = None\n",
    "            with open(model_path + '/imputers/smalls_' + col +'.pickle', 'rb') as handle:\n",
    "                frequent_cat = pickle.load(handle)\n",
    "                \n",
    "            cat[col] = np.where(cat[col].isin(frequent_cat), cat[col], 'smalls')\n",
    "        \n",
    "    cat_cols = cat.columns\n",
    "    ohe_cols = ohe.get_feature_names(cat_cols.values)\n",
    "    \n",
    "    df_cat = ohe.transform(cat.values)\n",
    "    df_cat = pd.DataFrame(df_cat, columns = ohe_cols, index = cat.index)\n",
    "    \n",
    "    return df_cat\n",
    "\n",
    "\n",
    "def process_cat(cat, params, model_path):\n",
    "    ohe = OneHotEncoder(sparse = False)\n",
    "    \n",
    "    cat = cat.replace('NA', np.nan)\n",
    "    cat = cat.replace('', np.nan)\n",
    "    cat = cat.replace(np.nan, 'NL')\n",
    "    \n",
    "    for col in cat.columns:\n",
    "        if col != params['date_feature']:\n",
    "            frequent_cat = find_non_rare_labels(cat, col, params['smalls'])\n",
    "            cat[col] = np.where(cat[col].isin(frequent_cat), cat[col], 'smalls')\n",
    "            \n",
    "            with open(model_path + '/imputers/smalls_' + col +'.pickle', 'wb') as handle:\n",
    "                pickle.dump(frequent_cat, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    cat_train, cat_val, cat_test = split_by_date(cat, params)\n",
    "    cat_cols = cat_train.columns\n",
    "    \n",
    "    ohe.fit(cat_train)\n",
    "    ohe.categories_ =  [append_smalls(cat) for cat in ohe.categories_]\n",
    "    \n",
    "    ohe_cols = ohe.get_feature_names(cat_cols.values)\n",
    "    \n",
    "    df_train_cat = ohe.transform(cat_train.values)\n",
    "    df_val_cat = ohe.transform(cat_val.values)\n",
    "    df_test_cat = ohe.transform(cat_test.values)\n",
    "    \n",
    "    df_train_cat = pd.DataFrame(df_train_cat, columns = ohe_cols, index = cat_train.index)\n",
    "    df_val_cat = pd.DataFrame(df_val_cat, columns = ohe_cols, index = cat_val.index)\n",
    "    df_test_cat = pd.DataFrame(df_test_cat, columns = ohe_cols, index = cat_test.index)\n",
    "    \n",
    "    with open(model_path + '/encoders/one_hot_encoder.pickle', 'wb') as handle:\n",
    "        pickle.dump(ohe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return df_train_cat, df_val_cat, df_test_cat\n",
    "\n",
    "def clean_text(text_input):\n",
    "    cleaner = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "    text = [\"\" if x is None else x.lower().strip() for x in text_input]\n",
    "    tokenized = [word_tokenize(x) for x in text]\n",
    "    tokenized = [(filter(None, x)) for x in tokenized]\n",
    "    cleaned_tokenized = [cleaner.tokenize(\" \".join(x)) for x in tokenized]\n",
    "    filtered_tokenized = [[s for s in x if s not in stop_words] for x in cleaned_tokenized]\n",
    "    sentences = [\" \".join(row) for row in filtered_tokenized]\n",
    "\n",
    "    return pd.Series(data = sentences, index = text_input.index)\n",
    "\n",
    "def process_nlp(nlp, params, prediction_only = False):\n",
    "    for col in nlp.columns:\n",
    "        if col != params['date_feature']:\n",
    "            nlp[col] = clean_text(nlp[col])\n",
    "            \n",
    "    if prediction_only:\n",
    "        return nlp\n",
    "    \n",
    "    nlp_train, nlp_val, nlp_test = split_by_date(nlp, params)\n",
    "    \n",
    "    return nlp_train, nlp_val, nlp_test\n",
    "\n",
    "def process_data_train(in_df, params, model_path):\n",
    "    df = in_df.copy()\n",
    "    df.set_index(params['index_features'], inplace = True)\n",
    "    df[params['date_feature']] = pd.to_datetime(df[params['date_feature']])\n",
    "    df[params['target_feature']] = df[params['target_feature']].astype(np.float64)\n",
    "    \n",
    "    df = remove_dates(df, params)\n",
    "    \n",
    "    util = df[params['utility_features'] + [params['target_feature'], params['date_feature']]].copy()\n",
    "    nlp = df[params['nlp_features'] + [params['date_feature']]].copy()\n",
    "    cat = df[params['categoric_features'] + [params['date_feature']]].copy()\n",
    "    num = df[params['numeric_features'] + [params['date_feature']]].copy()\n",
    "    \n",
    "    \n",
    "    cat_train, cat_val, cat_test = process_cat(cat, params, model_path)\n",
    "    num_train, num_val, num_test = process_num(num, params, model_path)   \n",
    "    nlp_train, nlp_val, nlp_test = process_nlp(nlp, params)\n",
    "    util_train, util_val, util_test = split_by_date(util, params, drop_date = False)\n",
    "    \n",
    "    train = pd.concat([util_train, cat_train, nlp_train, num_train], axis = 1)\n",
    "    val = pd.concat([util_val, cat_val, nlp_val, num_val], axis = 1)\n",
    "    test = pd.concat([util_test, cat_test, nlp_test, num_test], axis = 1)\n",
    "    \n",
    "    return train, val, test \n",
    "\n",
    "def process_data_predict(in_df, params, model_path):\n",
    "    df = in_df.copy()\n",
    "    df.set_index(params['index_features'], inplace = True)\n",
    "    df[params['date_feature']] = pd.to_datetime(df[params['date_feature']])\n",
    "    df[params['target_feature']] = df[params['target_feature']].astype(np.float64)\n",
    "    \n",
    "    util = df[params['utility_features'] + [params['target_feature'], params['date_feature']]].copy()\n",
    "    nlp = df[params['nlp_features'] + [params['date_feature']]].copy()\n",
    "    cat = df[params['categoric_features'] + [params['date_feature']]].copy()\n",
    "    num = df[params['numeric_features'] + [params['date_feature']]].copy()\n",
    "    \n",
    "    \n",
    "    cat_pred = process_cat_preds(cat, params, model_path)\n",
    "    num_pred = process_num_preds(num, params, model_path)   \n",
    "    nlp_pred = process_nlp(nlp, params, prediction_only = True)\n",
    "    util_pred = util\n",
    "    \n",
    "    pred = pd.concat([util_pred, cat_pred, nlp_pred, num_pred], axis = 1)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def process_data(in_df, params, model_path, prediction_only = False):\n",
    "    nltk.data.path.append(params['nltk_data_dir'])\n",
    "    if prediction_only:\n",
    "        return process_data_predict(in_df, params, model_path)\n",
    "    \n",
    "    return process_data_train(in_df, params, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all data and save imputers, etc\n",
    "train, val, test = process_data(all_data, process_config, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trains Model and output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seqs_pred(text, col, params, model_path):\n",
    "    tokenizer = None\n",
    "    with open(model_path + '/tokenizers/text_tokenizer_' + col + '.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "        \n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    sequences_padded = sequence.pad_sequences(sequences, maxlen = params['max_length'], padding='post', truncating='post')\n",
    "    \n",
    "    return sequences_padded  \n",
    "    \n",
    "def build_seqs(text_train, text_val, text_test, col, params, model_path):\n",
    "    tokenizer = Tokenizer(num_words=params['max_words'], oov_token = '<OOV>')\n",
    "    tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(text_train)\n",
    "    train_sequences_padded = sequence.pad_sequences(train_sequences, maxlen=params['max_length'], padding='post', truncating='post')\n",
    "    \n",
    "    val_sequences = tokenizer.texts_to_sequences(text_val)\n",
    "    val_sequences_padded = sequence.pad_sequences(val_sequences, maxlen=params['max_length'], padding='post', truncating='post')\n",
    "\n",
    "    test_sequences = tokenizer.texts_to_sequences(text_test)\n",
    "    test_sequences_padded = sequence.pad_sequences(test_sequences, maxlen=params['max_length'], padding='post', truncating='post')\n",
    "    \n",
    "    with open(model_path + '/tokenizers/text_tokenizer_' + col + '.pickle', 'wb') as handle:\n",
    "                pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return train_sequences_padded, val_sequences_padded, test_sequences_padded    \n",
    "    \n",
    "def make_callbacks(params):\n",
    "    early_stp = EarlyStopping(monitor='val_mse', min_delta=100, \n",
    "                            patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "    reducer = ReduceLROnPlateau(monitor=\"loss\", factor=0.1, patience=2, min_delta=100, mode='min')\n",
    "    decay = LearningRateScheduler(scheduler)\n",
    "    if params['lr_modifier'] == 'decay':\n",
    "        return [early_stp, decay]\n",
    "    elif params['lr_modifier'] == 'reducer':\n",
    "        return [early_stp, reducer]\n",
    "    else:\n",
    "        return [early_stp]\n",
    "    \n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr/epoch\n",
    "\n",
    "def build_text_model(inp, params):\n",
    "    text_model = (Embedding(params['max_words'], params['embedding_dim'], input_length = params['max_length']))(inp)\n",
    "    \n",
    "    for layer_num in np.arange(0, params['n_layers_gru']):\n",
    "        layer_size = int(params['embedding_dim'] / (2**layer_num))\n",
    "        text_model = (GRU(layer_size, \n",
    "                          return_sequences = not (layer_num == (params['n_layers_gru'] - 1))))(text_model)\n",
    "        \n",
    "    text_model = (Dense(8, activation = params['activation']))(text_model)\n",
    "    \n",
    "    return text_model\n",
    "    \n",
    "def build_nlp_model(train_shape, nlp_len, params):\n",
    "    inp_feats = Input(shape = (train_shape,))\n",
    "    inp_tknzd = lambda: Input(shape = (params['max_length'], ))\n",
    "\n",
    "    model = (Dense(2**(params['n_layers'] + 2), activation = params['activation']))(inp_feats)\n",
    "    model = (Dropout(params['dropout']))(model)\n",
    "        \n",
    "    for layer_num in np.flip(np.arange(2, params['n_layers'] + 1)):\n",
    "        layer_size = 2**(layer_num + 1)\n",
    "        model = (Dense(layer_size, activation = params['activation']))(model)\n",
    "        if layer_size > 100:\n",
    "            model = (Dropout(params['dropout']))(model)\n",
    "\n",
    "    models = [model]  \n",
    "    \n",
    "    # Building as many text models as there are in nlp seqs    \n",
    "    text_inputs = [inp_tknzd() for n in range(nlp_len)]\n",
    "    text_models = [build_text_model(inp, params) for inp in text_inputs]\n",
    "    \n",
    "    model = concatenate(models + text_models)\n",
    "    model = (Dense(1, activation=params['output_activation']))(model)\n",
    "    model = Model(inputs = [inp_feats] + text_inputs, outputs = model)\n",
    "    \n",
    "    optimizer = None\n",
    "    if params['optimizer'] =='SGD':\n",
    "        optimizer=SGD(learning_rate=params['learning_rate'])\n",
    "    elif params['optimizer'] =='Adam':\n",
    "        optimizer=Adam(learning_rate=params['learning_rate'])\n",
    "    elif params['optimizer'] =='RMSprop':\n",
    "        optimizer=RMSprop(learning_rate=params['learning_rate'])\n",
    "    elif params['optimizer'] =='Adagrad':\n",
    "        optimizer=Adagrad(learning_rate=params['learning_rate'])\n",
    "    elif params['optimizer'] =='Adamax':\n",
    "        optimizer=Adamax(learning_rate=params['learning_rate'])\n",
    "    elif params['optimizer'] =='Nadam':\n",
    "        optimizer=Nadam(learning_rate=params['learning_rate'])\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def split_data_by_type(df, process_config):\n",
    "    X = df[process_config['numeric_features'] + process_config['cat_ohe_features']].copy()\n",
    "    y = df[process_config['target_feature']].copy()\n",
    "    nlp = df[process_config['nlp_features']].copy()\n",
    "    util = df[process_config['utility_features'] + [process_config['date_feature']]].copy()\n",
    "    \n",
    "    return X, y, nlp, util\n",
    " \n",
    "def prep_data_pred(pred, model_path, process_config, model_config):\n",
    "    process_config['cat_ohe_features'] = [\n",
    "        x for x in pred.columns if any([y in x for y in process_config['categoric_features']])]\n",
    "    \n",
    "    X, y, nlp, util = split_data_by_type(pred, process_config)\n",
    "        \n",
    "    nlp_seqs = []\n",
    "    for col in nlp.columns:\n",
    "        seq = build_seqs_pred(nlp[col], col, model_config, model_path)\n",
    "        nlp_seqs.append(seq)\n",
    "        \n",
    "    return (X, y, nlp_seqs, util)\n",
    "    \n",
    "    \n",
    "def prep_data(train, val, test, model_path, process_config, model_config):\n",
    "    process_config['cat_ohe_features'] = [\n",
    "        x for x in train.columns if any([y in x for y in process_config['categoric_features']])]\n",
    "    \n",
    "    X_train, y_train, nlp_train, util_train = split_data_by_type(train, process_config)\n",
    "    X_val, y_val, nlp_val, util_val = split_data_by_type(val, process_config)\n",
    "    X_test, y_test, nlp_test, util_test =  split_data_by_type(test, process_config)\n",
    "    \n",
    "    nlp_seqs_train = []\n",
    "    nlp_seqs_val = []\n",
    "    nlp_seqs_test = []\n",
    "    for col in nlp_train.columns:\n",
    "        seq_train, seq_val, seq_test = build_seqs(nlp_train[col], nlp_val[col], nlp_test[col], col, model_config, model_path)\n",
    "        nlp_seqs_train.append(seq_train)\n",
    "        nlp_seqs_val.append(seq_val)\n",
    "        nlp_seqs_test.append(seq_test)\n",
    "        \n",
    "    return (X_train, y_train, nlp_seqs_train, util_train,\n",
    "            X_val, y_val, nlp_seqs_val, util_val,\n",
    "            X_test, y_test, nlp_seqs_test, util_test)\n",
    "\n",
    "def run_model(train, val, test, model_path, process_config, model_config):\n",
    "    X_train, y_train, nlp_seqs_train, util_train, \\\n",
    "    X_val, y_val, nlp_seqs_val, util_val, \\\n",
    "    X_test, y_test, nlp_seqs_test, util_test = prep_data(train, val, test, model_path, process_config, model_config)\n",
    "    \n",
    "    model = build_nlp_model(X_train.shape[1], len(nlp_seqs_train), model_config)\n",
    "    model.fit(\n",
    "        [X_train] + nlp_seqs_train, \n",
    "        y_train, \n",
    "        validation_data = ([X_val] + nlp_seqs_val, y_val),\n",
    "        epochs = model_config['epochs'], \n",
    "        batch_size = model_config['batch_size'],\n",
    "        callbacks = make_callbacks(model_config), \n",
    "        verbose = model_config['verbose']\n",
    "    )\n",
    "    \n",
    "    model.save(model_path + \"/model\")\n",
    "    \n",
    "    train_preds = model.predict([X_train] + nlp_seqs_train)\n",
    "    val_preds = model.predict([X_val] + nlp_seqs_val)\n",
    "    test_preds = model.predict([X_test] + nlp_seqs_test)\n",
    "        \n",
    "    util_train['target'] = y_train\n",
    "    util_val['target'] = y_val\n",
    "    util_test['target'] = y_test\n",
    "    \n",
    "    util_train['prediction'] = train_preds\n",
    "    util_val['prediction'] = val_preds\n",
    "    util_test['prediction'] = test_preds\n",
    "    \n",
    "    return model, util_train, util_val, util_test\n",
    "\n",
    "def run_model_predict(pred, model_path, process_config, model_config):\n",
    "    X, y, nlp_seqs, util = prep_data_pred(pred, model_path, process_config, model_config)\n",
    "\n",
    "    model = tf.keras.models.load_model(model_path + \"/model\")\n",
    "    \n",
    "    preds = model.predict([X] + nlp_seqs)\n",
    "        \n",
    "    util['target'] = y\n",
    "    util['prediction'] = preds\n",
    "    \n",
    "    return model, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, out_train, out_val, out_test = run_model(train, val, test, model_path, process_config, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gets Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_cov(x, y, w):\n",
    "    return np.sum(w * (x - np.average(x, weights = w)) * (y - np.average(y, weights = w))) / np.sum(w)\n",
    "def w_corr(x, y, w):\n",
    "    return w_cov(x, y, w) / np.sqrt(w_cov(x, x, w)*w_cov(y, y, w))\n",
    "def get_weighted_corr(df, pred, truth):\n",
    "    return w_corr(df[pred], df[truth], (df['calls_off'] + df['calls_on']))\n",
    "    \n",
    "def get_overall_lift(gain_df):\n",
    "    overall_lift = gain_df.groupby('v_calltime_dt').agg(\n",
    "        pred_incrs = ('prediction_incr', np.sum),\n",
    "        true_incrs = ('target_incr', np.sum),\n",
    "        calls_on = ('calls_on', np.sum),\n",
    "        calls_off = ('calls_off', np.sum)\n",
    "    )\n",
    "    overall_lift['pred_lift'] = overall_lift['pred_incrs'] / overall_lift['calls_on']\n",
    "    overall_lift['true_lift'] = overall_lift['true_incrs'] / overall_lift['calls_on']\n",
    "    return overall_lift\n",
    "    \n",
    "def get_validation_data(out_train, out_val, out_test):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    val_data = {}\n",
    "    \n",
    "    # RMSES\n",
    "    val_data['train_rmse'] = mean_squared_error(out_train['target'], out_train['prediction'], squared = False)\n",
    "    val_data['validation_rmse'] = mean_squared_error(out_val['target'], out_val['prediction'], squared = False)\n",
    "    \n",
    "    return val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validations\n",
    "val_data = get_validation_data(out_train, out_val, out_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model data into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metadata(metadata, model_info_config):\n",
    "    model_data_update = model_info_config['insert_model_info_query'].format(**metadata)\n",
    "    model_info_conn = pymysql.connect(**model_info_config['db_args'])\n",
    "    \n",
    "    cur = model_info_conn.cursor()\n",
    "    cur.execute('SET autocommit = 1')\n",
    "    cur.execute('set sql_safe_updates = 0')\n",
    "    cur.execute(model_data_update)\n",
    "    cur.close()\n",
    "    model_info_conn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get, and upload metadata\n",
    "metadata = {\n",
    "    **model_info_config,\n",
    "    **val_data,\n",
    "    **process_config,\n",
    "    'model_trained_on': model_train_date,\n",
    "    'model_expires_on': model_expires_date,\n",
    "    'model_output_base_dir': model_path\n",
    "}\n",
    "update_metadata(metadata, model_info_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_preds(pred_data, process_config, model_info_config, pipeline_config):\n",
    "    model_info_conn = pymysql.connect(**model_info_config['db_args'])\n",
    "    cur = model_info_conn.cursor()\n",
    "    cur.execute('SET autocommit = 1')\n",
    "    cur.execute('set sql_safe_updates = 0')\n",
    "    \n",
    "    for index, row in pred_data.reset_index().iterrows():\n",
    "        output_data = dict(zip(\n",
    "            process_config['index_features'], \n",
    "            row[process_config['index_features']]\n",
    "        ))\n",
    "        output_data ['prediction'] = row['prediction']\n",
    "        insert_query = pipeline_config['output_query'].format(\n",
    "            **output_data,\n",
    "            **pipeline_config\n",
    "        )\n",
    "        cur.execute(insert_query)\n",
    "    cur.close()\n",
    "    model_info_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload predictions\n",
    "upload_preds(out_test, process_config, model_info_config, pipeline_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
